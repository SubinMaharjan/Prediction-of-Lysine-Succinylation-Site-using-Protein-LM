{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install sentencepiece\n","!pip install transformers\n","import torch\n","# from transformers import AutoTokenizer, AutoModel, pipeline\n","# import re\n","from transformers import T5Tokenizer, T5EncoderModel\n","import numpy as np\n","import os\n","# import requests\n","from tqdm.auto import tqdm\n","#from numba import cuda\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# import torch_xla.core.xla_model as xm\n","# device = xm.xla_device()\n","device = torch.device(\"cuda:0\")\n","# device = torch.device(\"cpu\")\n","# model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\", do_lower_case = False)\n","# tokenizer = AutoTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case = False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# model = AutoModel.from_pretrained(\"Rostlab/prot_bert\") # load the model PROTBERT\n","# model = T5Model.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n","model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_half_uniref50-enc\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pandas as pd\n","path =  '/kaggle/input/final-data/'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["succinylation_negative = pd.read_csv(path + 'negative_sites_sequence.csv').dropna().drop_duplicates()\n","succinylation_positive = pd.read_csv(path + 'positive_sites_sequence.csv').dropna().drop_duplicates()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["succinylation_positive.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["positive_datasets = {}\n","negative_datasets = {}\n","positive_datasets['protein_position'] = succinylation_positive['site']\n","positive_datasets['protein_seq'] = succinylation_positive['protein_sequence']\n","negative_datasets['protein_position'] = succinylation_negative['site']\n","negative_datasets['protein_seq'] = succinylation_negative['protein_sequence']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["window_size = 15 # Actual : 100 + 1 + 100\n","def extract_embedding(encoded_input):\n","  embedding = None  \n","  with torch.no_grad():\n","    embedding = model(**encoded_input)[0]\n","    embeds = embedding.detach().to('cpu')\n","    del embedding\n","    torch.cuda.empty_cache()\n","    gc.collect()\n","\n","  return embeds\n","\n","from copy import deepcopy\n","def clip_features(embedding, sequence, position):\n","  sequence = sequence.replace(' ','')\n","  prot_pos = position-1 \n","  feature = embedding[prot_pos-window_size: prot_pos+window_size+1]\n","  return feature, position\n","\n","\n","def generate_embeddings(sequences, positions):\n","  encoded_input = None\n","  embedding = None\n","  generated_embeddings = []\n","  new_positions = []\n","  for i, sequence in enumerate(sequences):\n","#     if i < 430:continue\n","    if i % 100 == 0: print(\"Batch_Size: \",i)\n","    position = positions[i]\n","    if position > len(sequence): continue\n","    if len(sequence) > 4900:\n","        if position < 4900:\n","            sequence = sequence[:4900-1]\n","        else:\n","            sequence = sequence[4900-1:]\n","            position = position-4900+1       \n","            \n","    new_sequence = ''\n","    prot_pos = position-1            \n","    \n","    if prot_pos - window_size < 0 and len(sequence[prot_pos+1:]) < window_size:\n","        new_sequence = 'X'*(window_size-prot_pos) + sequence + 'X'*(window_size-len(sequence[prot_pos+1:])) \n","        position = 16\n","    elif prot_pos - window_size < 0:\n","#         print(\"num\",prot_pos, window_size)\n","        new_sequence = 'X'*(window_size-prot_pos) + sequence\n","        position = 16\n","    elif len(sequence[prot_pos+1:]) < window_size:\n","        new_sequence = sequence + 'X'*(window_size-len(sequence[prot_pos+1:])) \n","    else:\n","        new_sequence = sequence\n","    sequence = \" \".join(new_sequence) #Adding spacing between each character as rostlab takes spaces between each word\n","#     print(\"sequence_length: \", len(new_sequence))\n","    encoded_input = tokenizer(sequence,return_tensors='pt').to(device)\n","#     encoded_input = tokenizer.batch_encode_plus(sequence, add_special_tokens=True, padding=\"longest\")\n","    embedding = extract_embedding(encoded_input)\n","    embedding =feature_extractor(embedding, sequence)\n","#     print(embedding.shape)\n","    ##### Write a code to extract 200 positions \n","    feature,position = clip_features(embedding, sequence, position)\n","#     print(embedding.shape, feature.shape[0], i, position)\n","    assert feature.shape[0] == 31\n","    generated_embeddings.append(feature.numpy())\n","    new_positions.append(position)\n","    del encoded_input\n","    del embedding                 \n","    torch.cuda.empty_cache()\n","    gc.collect()   \n","\n","  return generated_embeddings, new_positions\n","\n","\n","def feature_extractor(embedding, sequence):\n","  seq_emb = embedding\n","#   print(seq_emb[0].shape)\n","  seq_emb = seq_emb[0][:len(sequence)] \n","  return seq_emb\n","\n","\n","def save_file(filename, features):\n","    path = '/kaggle/working/'\n","#     filename = 'negative.npy'\n","#     torch.save(features, path+filename)\n","    np.savez(path + filename, features)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import math\n","neg_sequences = list(negative_datasets['protein_seq'])\n","neg_sequences_position = list(negative_datasets['protein_position'])\n","no_of_iter = math.ceil(len(neg_sequences)/500)\n","neg_pos_list = []\n","for i in range(no_of_iter):\n","    print(\"No_of_iterations:\", i+1)\n","    neg_embeddings, neg_position = generate_embeddings(neg_sequences[:500], neg_sequences_position[:500])\n","    neg_pos_list.extend(neg_position)\n","    save_file('negative'+str(i)+'.npz', neg_embeddings)\n","    neg_sequences = neg_sequences[500:]\n","    neg_sequences_position = neg_sequences_position[500:]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import math\n","pos_sequences = list(positive_datasets['protein_seq'])\n","pos_sequences_position = list(positive_datasets['protein_position'])\n","no_of_iter = math.ceil(len(pos_sequences)/500)\n","pos_pos_list = []\n","for i in range(no_of_iter):\n","    print(\"No_of_iterations:\", i+1)\n","    pos_embeddings, pos_position = generate_embeddings(pos_sequences[:500], pos_sequences_position[:500])\n","    pos_pos_list.extend(pos_position)\n","    pos_sequences = pos_sequences[500:]\n","    pos_sequences_position = pos_sequences_position[500:]\n","    save_file('positive'+str(i)+'.npz', pos_embeddings)\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["path = '/kaggle/working/'\n","df = pd.DataFrame({'position': pos_pos_list})\n","df.to_csv(path+'positive_position.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
